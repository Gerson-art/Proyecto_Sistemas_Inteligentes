<!DOCTYPE html>
<html lang="es">

<head>
  <meta charset="UTF-8">
  <title>Reinforcement Learning</title>
  <link rel="stylesheet" href="/css/rl.css">
  <script src="/js/rl.js" defer></script>
</head>

<body>
  <section id="reinforcement-learning">
    <div class="section-header">
      <h1>Reinforcement Learning</h1>
      <div class="subtitle">Aprendizaje por Refuerzo: agente, entorno, pol√≠tica y recompensas</div>
    </div>

    <!-- Ejemplo de c√≥mo mejorar la secci√≥n de Fundamentos -->
    <div class="card fundamentos">
      <h2><span class="icon">ü§ñ</span> Fundamentos del Aprendizaje por Refuerzo</h2>
      <div class="content">
        <div class="teoria-decision">
          <div class="caracteristicas">
            <div class="caracteristica item-nodo">
              <div class="contenido">
                <p>El <strong>Aprendizaje por Refuerzo</strong> (RL) es una t√©cnica donde un <strong>agente</strong>
                  aprende a tomar decisiones √≥ptimas interactuando con un <strong>entorno</strong>. A diferencia del
                  aprendizaje supervisado, no recibe respuestas correctas, sino que descubre estrategias mediante <span
                    class="highlight">recompensas</span>.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Ejemplo de c√≥mo mejorar la secci√≥n de Exploraci√≥n vs Explotaci√≥n -->
    <div class="card">
      <h2><span class="icon">‚öñÔ∏è</span> Exploraci√≥n vs Explotaci√≥n</h2>
      <div class="content">
        <div class="teoria-decision">
          <div class="caracteristicas">
            <div class="caracteristica item-rama">
              <div class="contenido">
                <h3>El Dilema Fundamental</h3>
                <p>El agente debe equilibrar dos estrategias opuestas pero complementarias:</p>
              </div>
            </div>

            <div class="caracteristica item-hoja">
              <div class="contenido">
                <h3>Exploraci√≥n</h3>
                <p>Probar nuevas acciones para descubrir recompensas potencialmente mejores.</p>
              </div>
            </div>

            <div class="caracteristica item-hoja">
              <div class="contenido">
                <h3>Explotaci√≥n</h3>
                <p>Elegir las acciones que ya conoce como efectivas para maximizar recompensas inmediatas.</p>
              </div>
            </div>

            <div class="caracteristica item-nodo">
              <div class="contenido">
                <h3>Balance Necesario</h3>
                <p>Este balance se gestiona com√∫nmente con el par√°metro <code>Œµ</code> (epsilon) en algoritmos Œµ-greedy,
                  que determina la probabilidad de explorar versus explotar.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="card conceptos-clave">
      <h2><span class="icon">üìò</span> Conceptos Clave</h2>
      <div class="content grid-conceptos">
        <div class="concepto-item">
          <div class="concepto-icon">üßë‚Äçüíª</div>
          <div>
            <h3>Agente</h3>
            <p>Quien toma decisiones y aprende para maximizar recompensas.</p>
          </div>
        </div>
        <div class="concepto-item">
          <div class="concepto-icon">üåç</div>
          <div>
            <h3>Entorno</h3>
            <p>El mundo con el que interact√∫a el agente.</p>
          </div>
        </div>
        <div class="concepto-item">
          <div class="concepto-icon">üìä</div>
          <div>
            <h3>Estado</h3>
            <p>Representaci√≥n del entorno en un momento dado.</p>
          </div>
        </div>
        <div class="concepto-item">
          <div class="concepto-icon">üéØ</div>
          <div>
            <h3>Acci√≥n</h3>
            <p>Elecci√≥n que hace el agente en cada estado.</p>
          </div>
        </div>
        <div class="concepto-item">
          <div class="concepto-icon">üèÜ</div>
          <div>
            <h3>Recompensa</h3>
            <p>Valor que indica qu√© tan buena fue una acci√≥n.</p>
          </div>
        </div>
        <div class="concepto-item">
          <div class="concepto-icon">üìú</div>
          <div>
            <h3>Pol√≠tica</h3>
            <p>Estrategia que define qu√© acci√≥n tomar en cada estado.</p>
          </div>
        </div>
      </div>
    </div>

    <div class="card ciclo-aprendizaje">
      <h2><span class="icon">üîÑ</span> Ciclo de Aprendizaje por Refuerzo</h2>
      <div class="content">
        <div class="learning-cycle-container">
          <div class="cycle-steps">
            <div class="cycle-step">
              <div class="step-visual">
                <div class="step-number">1</div>
                <div class="step-connector"></div>
              </div>
              <div class="step-content">
                <h3>Observaci√≥n del Estado</h3>
                <p>El agente analiza su situaci√≥n actual en el entorno.</p>
                <div class="step-icon">üëÄ</div>
              </div>
            </div>

            <div class="cycle-step">
              <div class="step-visual">
                <div class="step-number">2</div>
                <div class="step-connector"></div>
              </div>
              <div class="step-content">
                <h3>Selecci√≥n de Acci√≥n</h3>
                <p>Elige un movimiento basado en su pol√≠tica actual.</p>
                <div class="step-icon">üéØ</div>
              </div>
            </div>

            <div class="cycle-step">
              <div class="step-visual">
                <div class="step-number">3</div>
                <div class="step-connector"></div>
              </div>
              <div class="step-content">
                <h3>Recepci√≥n de Recompensa</h3>
                <p>El entorno devuelve feedback sobre la acci√≥n tomada.</p>
                <div class="step-icon">üí∞</div>
              </div>
            </div>

            <div class="cycle-step">
              <div class="step-visual">
                <div class="step-number">4</div>
              </div>
              <div class="step-content">
                <h3>Actualizaci√≥n de Pol√≠tica</h3>
                <p>El agente ajusta su estrategia para futuras decisiones.</p>
                <div class="step-icon">üìà</div>
              </div>
            </div>
          </div>


        </div>
      </div>
    </div>

    <!-- Agrega esto dentro de <section id="reinforcement-learning"> -->
    <div class="card">
      <h2><span class="icon">üéÆ</span> Laberinto Interactivo (Q-Learning)</h2>
      <div class="content">
        <div id="maze-container">
          <canvas id="maze-canvas" width="400" height="400"></canvas>
          <div class="controls">
            <div class="param-controls">
              <div class="param-group">
                <label for="epsilon">Exploraci√≥n (Œµ):
                  <span id="epsilon-value">0.4</span>
                  <input type="range" id="epsilon" min="0" max="1" step="0.05" value="0.4">
                </label>
              </div>
              <div class="param-group">
                <label for="alpha">Tasa aprendizaje (Œ±):
                  <span id="alpha-value">0.6</span>
                  <input type="range" id="alpha" min="0" max="1" step="0.05" value="0.6">
                </label>
              </div>
              <div class="param-group">
                <label for="gamma">Factor descuento (Œ≥):
                  <span id="gamma-value">0.9</span>
                  <input type="range" id="gamma" min="0" max="1" step="0.05" value="0.9">
                </label>
              </div>
            </div>
            <div class="action-buttons">
              <button id="train-btn">Entrenar Agente</button>
              <button id="reset-btn">Reiniciar</button>
              <button id="show-policy-btn">Mostrar Pol√≠tica</button>
            </div>
          </div>
        </div>
        <div class="policy-visualization">
          <h3>Visualizaci√≥n de Pol√≠tica</h3>
          <div id="policy-display"></div>
        </div>
      </div>
    </div>
    <div class="feedback-container">
      <div class="feedback-card">
        <div class="feedback-header">
          <h3>Progreso del Entrenamiento</h3>
          <div class="status-indicator" id="training-status"></div>
        </div>

        <div class="feedback-metrics">
          <div class="metric">
            <div class="metric-label">Episodio</div>
            <div class="metric-value" id="episode-counter">0</div>
          </div>

          <div class="metric">
            <div class="metric-label">Pasos</div>
            <div class="metric-value" id="step-counter">0</div>
            <div class="metric-description">en este episodio</div>
          </div>
        </div>
      </div>
    </div>

    <div class="card">
      <h2><span class="icon">üíª</span> Implementaci√≥n con Q-Learning</h2>
      <div class="content">
        <div class="code-explanation">
          <div class="explanation-header">
            <span class="icon">üìù</span>
            <h3>Explicaci√≥n del C√≥digo Clave</h3>
          </div>

          <div class="explanation-grid">
            <!-- Paso 1 -->
            <div class="code-step">
              <div class="step-header">
                <div class="step-number">1</div>
                <h4>Definici√≥n del Entorno</h4>
              </div>
              <pre><code class="language-javascript">// Matriz que representa el laberinto:
// 0: celda vac√≠a, 1: pared, 2: meta
const maze = [
  [0, 0, 1, 2],
  [1, 0, 1, 0],
  [0, 0, 0, 0]
];</code></pre>
              <p>El entorno se define como una matriz donde cada celda puede ser:
                <span class="code-comment">espacio transitable (0)</span>,
                <span class="code-comment">pared/bloqueo (1)</span> o
                <span class="code-comment">meta (2)</span>.
              </p>
            </div>

            <!-- Paso 2 -->
            <div class="code-step">
              <div class="step-header">
                <div class="step-number">2</div>
                <h4>Inicializaci√≥n de la Tabla Q</h4>
              </div>
              <pre><code class="language-javascript">// Inicializa la tabla Q con ceros para cada estado-acci√≥n
function initializeQ() {
  Q = {};
  for (let y = 0; y < rows; y++) {
    for (let x = 0; x < cols; x++) {
      if (maze[y][x] !== 1) { // Ignora paredes
        Q[`${y},${x}`] = [0, 0, 0, 0]; // [‚Üë, ‚Üí, ‚Üì, ‚Üê]
      }
    }
  }
}</code></pre>
              <p>Crea una tabla Q donde cada estado (posici√≥n) tiene un array de 4 valores (uno por acci√≥n posible),
                inicializados en 0.</p>
            </div>

            <!-- Paso 3 -->
            <div class="code-step">
              <div class="step-header">
                <div class="step-number">3</div>
                <h4>Selecci√≥n de Acciones (Œµ-greedy)</h4>
              </div>
              <pre><code class="language-javascript">function chooseAction(state) {
  const key = `${state[0]},${state[1]}`;
  if (Math.random() < epsilon) { // Exploraci√≥n
    return Math.floor(Math.random() * 4);
  } else { // Explotaci√≥n
    const q = Q[key];
    const max = Math.max(...q);
    const options = q.map((val, idx) => 
      val === max ? idx : -1
    ).filter(i => i !== -1);
    return options[Math.floor(Math.random() * options.length)];
  }
}</code></pre>
              <p>Con probabilidad Œµ elige una acci√≥n aleatoria (exploraci√≥n), de lo contrario elige la mejor acci√≥n
                conocida (explotaci√≥n). Si hay empate, elige aleatoriamente entre las mejores.</p>
            </div>

            <!-- Paso 4 -->
            <div class="code-step">
              <div class="step-header">
                <div class="step-number">4</div>
                <h4>Funci√≥n de Recompensa</h4>
              </div>
              <pre><code class="language-javascript">function getReward(y, x) {
  if (maze[y][x] === 2) return 20;  // Meta
  if (maze[y][x] === 1) return -10; // Pared
  return -1;                        // Paso normal
}</code></pre>
              <p>Define las recompensas: gran premio por alcanzar la meta, penalizaci√≥n por chocar con paredes y peque√±o
                costo por movimiento.</p>
            </div>

            <!-- Paso 5 -->
            <div class="code-step">
              <div class="step-header">
                <div class="step-number">5</div>
                <h4>Actualizaci√≥n Q-Learning</h4>
              </div>
              <pre><code class="language-javascript">function trainStep() {
  // 1. Obtener estado actual
  let state = [...agentPos];
  const stateKey = `${state[0]},${state[1]}`;
  
  // 2. Elegir y ejecutar acci√≥n
  const action = chooseAction(state);
  const [dy, dx] = actions[action];
  const newY = state[0] + dy;
  const newX = state[1] + dx;
  
  if (!isValid(newY, newX)) return;
  
  // 3. Calcular recompensa y nuevo estado
  const nextStateKey = `${newY},${newX}`;
  const reward = getReward(newY, newX);
  
  // 4. Actualizar valor Q (ecuaci√≥n de Bellman)
  const maxQ = Math.max(...Q[nextStateKey]);
  Q[stateKey][action] += alpha * (
    reward + gamma * maxQ - Q[stateKey][action]
  );
  
  // 5. Mover agente y actualizar UI
  agentPos = [newY, newX];
  updateUI();
}</code></pre>
              <p>Implementa el n√∫cleo del algoritmo Q-Learning, actualizando los valores Q usando la ecuaci√≥n de
                Bellman.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <footer class="section-footer">
    <div class="container">
      <nav>
        <a href="/" class="btn outline">‚Üê Volver a la Portada</a>
        <a href="/arboles.html" class="btn">√Årbol de decisi√≥n ‚Üí</a>
      </nav>
    </div>
  </footer>

</body>

</html>